{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dendrites without Perforated Backpropagation\n",
      "[DEBUG] Using device: cpu\n",
      "[DEBUG] Loading dataset...\n",
      "[DEBUG] Original Train Size: 12,774,667\n",
      "[DEBUG] Train positive ratio: 0.5\n",
      "[DEBUG] Val positive ratio: 0.5\n",
      "[DEBUG] Test positive ratio: 0.5\n",
      "\n",
      "============================================================\n",
      "RUNNING WITH is_dendrite = False\n",
      "============================================================\n",
      "[DEBUG] Running BASELINE...\n",
      "[DEBUG] Parameter count: 4,545\n",
      "Epoch 1\n",
      "  Train Acc: 53.48%\n",
      "  Val Acc: 55.72%\n",
      "  Precision: 0.5389\n",
      "  Recall:    0.7914\n",
      "  F1 Score:  0.6412\n",
      "Epoch 2\n",
      "  Train Acc: 57.44%\n",
      "  Val Acc: 57.38%\n",
      "  Precision: 0.5517\n",
      "  Recall:    0.7876\n",
      "  F1 Score:  0.6489\n",
      "Epoch 3\n",
      "  Train Acc: 58.64%\n",
      "  Val Acc: 57.81%\n",
      "  Precision: 0.5825\n",
      "  Recall:    0.5512\n",
      "  F1 Score:  0.5664\n",
      "Epoch 4\n",
      "  Train Acc: 58.90%\n",
      "  Val Acc: 56.85%\n",
      "  Precision: 0.5454\n",
      "  Recall:    0.8222\n",
      "  F1 Score:  0.6558\n",
      "Epoch 5\n",
      "  Train Acc: 59.01%\n",
      "  Val Acc: 58.27%\n",
      "  Precision: 0.5683\n",
      "  Recall:    0.6877\n",
      "  F1 Score:  0.6223\n",
      "Epoch 6\n",
      "  Train Acc: 59.17%\n",
      "  Val Acc: 58.17%\n",
      "  Precision: 0.5697\n",
      "  Recall:    0.6679\n",
      "  F1 Score:  0.6149\n",
      "Epoch 7\n",
      "  Train Acc: 59.34%\n",
      "  Val Acc: 58.12%\n",
      "  Precision: 0.5772\n",
      "  Recall:    0.6069\n",
      "  F1 Score:  0.5917\n",
      "Epoch 8\n",
      "  Train Acc: 59.71%\n",
      "  Val Acc: 58.46%\n",
      "  Precision: 0.5830\n",
      "  Recall:    0.5939\n",
      "  F1 Score:  0.5884\n",
      "Epoch 9\n",
      "  Train Acc: 59.73%\n",
      "  Val Acc: 58.70%\n",
      "  Precision: 0.5723\n",
      "  Recall:    0.6886\n",
      "  F1 Score:  0.6251\n",
      "Epoch 10\n",
      "  Train Acc: 59.55%\n",
      "  Val Acc: 58.19%\n",
      "  Precision: 0.5719\n",
      "  Recall:    0.6516\n",
      "  F1 Score:  0.6092\n",
      "Epoch 11\n",
      "  Train Acc: 59.78%\n",
      "  Val Acc: 58.27%\n",
      "  Precision: 0.5741\n",
      "  Recall:    0.6401\n",
      "  F1 Score:  0.6053\n",
      "Epoch 12\n",
      "  Train Acc: 59.72%\n",
      "  Val Acc: 58.34%\n",
      "  Precision: 0.5736\n",
      "  Recall:    0.6497\n",
      "  F1 Score:  0.6093\n",
      "Epoch 13\n",
      "  Train Acc: 59.64%\n",
      "  Val Acc: 58.31%\n",
      "  Precision: 0.5744\n",
      "  Recall:    0.6420\n",
      "  F1 Score:  0.6063\n",
      "Epoch 14\n",
      "  Train Acc: 59.64%\n",
      "  Val Acc: 58.24%\n",
      "  Precision: 0.5723\n",
      "  Recall:    0.6521\n",
      "  F1 Score:  0.6096\n",
      "Epoch 15\n",
      "  Train Acc: 59.74%\n",
      "  Val Acc: 58.43%\n",
      "  Precision: 0.5735\n",
      "  Recall:    0.6579\n",
      "  F1 Score:  0.6128\n",
      "Epoch 16\n",
      "  Train Acc: 59.61%\n",
      "  Val Acc: 58.46%\n",
      "  Precision: 0.5732\n",
      "  Recall:    0.6627\n",
      "  F1 Score:  0.6147\n",
      "Epoch 17\n",
      "  Train Acc: 59.71%\n",
      "  Val Acc: 58.41%\n",
      "  Precision: 0.5730\n",
      "  Recall:    0.6598\n",
      "  F1 Score:  0.6134\n",
      "Epoch 18\n",
      "  Train Acc: 59.70%\n",
      "  Val Acc: 58.46%\n",
      "  Precision: 0.5732\n",
      "  Recall:    0.6622\n",
      "  F1 Score:  0.6145\n",
      "Epoch 19\n",
      "  Train Acc: 59.70%\n",
      "  Val Acc: 58.51%\n",
      "  Precision: 0.5737\n",
      "  Recall:    0.6617\n",
      "  F1 Score:  0.6146\n",
      "Epoch 20\n",
      "  Train Acc: 59.75%\n",
      "  Val Acc: 58.31%\n",
      "  Precision: 0.5724\n",
      "  Recall:    0.6574\n",
      "  F1 Score:  0.6119\n",
      "\n",
      "========== FINAL TEST ==========\n",
      "Test Acc: 60.14%\n",
      "Precision: 0.5878\n",
      "Recall:    0.6793\n",
      "F1 Score:  0.6302\n",
      "\n",
      "============================================================\n",
      "RUNNING WITH is_dendrite = True\n",
      "============================================================\n",
      "[DEBUG] Initializing WITH dendrites...\n",
      "Running a test of Dendrite Capacity.\n",
      "[DEBUG] Parameter count: 9,090\n",
      "Epoch 1\n",
      "  Train Acc: 54.91%\n",
      "  Val Acc: 58.15%\n",
      "  Precision: 0.5774\n",
      "  Recall:    0.6074\n",
      "  F1 Score:  0.5920\n",
      "Adding validation score 58.14512254\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 0, last improved epoch 0, total epochs 0, n: 10, num_cycles: 0\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "  Train Acc: 57.96%\n",
      "  Val Acc: 56.80%\n",
      "  Precision: 0.5833\n",
      "  Recall:    0.4762\n",
      "  F1 Score:  0.5243\n",
      "Adding validation score 56.79961557\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 1, last improved epoch 1, total epochs 1, n: 10, num_cycles: 2\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "  Train Acc: 58.71%\n",
      "  Val Acc: 58.24%\n",
      "  Precision: 0.5715\n",
      "  Recall:    0.6588\n",
      "  F1 Score:  0.6121\n",
      "Adding validation score 58.24123018\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 2, last improved epoch 2, total epochs 2, n: 10, num_cycles: 4\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "  Train Acc: 59.05%\n",
      "  Val Acc: 58.34%\n",
      "  Precision: 0.5818\n",
      "  Recall:    0.5930\n",
      "  F1 Score:  0.5873\n",
      "Adding validation score 58.33733782\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 3, last improved epoch 3, total epochs 3, n: 10, num_cycles: 6\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "  Train Acc: 60.05%\n",
      "  Val Acc: 59.39%\n",
      "  Precision: 0.5709\n",
      "  Recall:    0.7568\n",
      "  F1 Score:  0.6508\n",
      "Adding validation score 59.39452186\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 4, last improved epoch 4, total epochs 4, n: 10, num_cycles: 8\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "  Train Acc: 61.81%\n",
      "  Val Acc: 62.28%\n",
      "  Precision: 0.6323\n",
      "  Recall:    0.5867\n",
      "  F1 Score:  0.6087\n",
      "Adding validation score 62.27775108\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 5, last improved epoch 5, total epochs 5, n: 10, num_cycles: 10\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "  Train Acc: 62.68%\n",
      "  Val Acc: 62.35%\n",
      "  Precision: 0.6169\n",
      "  Recall:    0.6516\n",
      "  F1 Score:  0.6338\n",
      "Adding validation score 62.34983181\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 6, last improved epoch 6, total epochs 6, n: 10, num_cycles: 12\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "  Train Acc: 63.08%\n",
      "  Val Acc: 62.16%\n",
      "  Precision: 0.6164\n",
      "  Recall:    0.6439\n",
      "  F1 Score:  0.6298\n",
      "Adding validation score 62.15761653\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 7, last improved epoch 7, total epochs 7, n: 10, num_cycles: 14\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "  Train Acc: 63.55%\n",
      "  Val Acc: 62.42%\n",
      "  Precision: 0.6407\n",
      "  Recall:    0.5656\n",
      "  F1 Score:  0.6008\n",
      "Adding validation score 62.42191254\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 8, last improved epoch 8, total epochs 8, n: 10, num_cycles: 16\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "  Train Acc: 64.07%\n",
      "  Val Acc: 62.25%\n",
      "  Precision: 0.6197\n",
      "  Recall:    0.6343\n",
      "  F1 Score:  0.6269\n",
      "Adding validation score 62.25372417\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 9, last improved epoch 9, total epochs 9, n: 10, num_cycles: 18\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "  Train Acc: 63.50%\n",
      "  Val Acc: 62.21%\n",
      "  Precision: 0.6113\n",
      "  Recall:    0.6704\n",
      "  F1 Score:  0.6395\n",
      "Adding validation score 62.20567035\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 10, last improved epoch 10, total epochs 10, n: 10, num_cycles: 20\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "  Train Acc: 63.71%\n",
      "  Val Acc: 62.52%\n",
      "  Precision: 0.6468\n",
      "  Recall:    0.5517\n",
      "  F1 Score:  0.5954\n",
      "Adding validation score 62.51802018\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 11, last improved epoch 11, total epochs 11, n: 10, num_cycles: 22\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n",
      "  Train Acc: 64.07%\n",
      "  Val Acc: 62.97%\n",
      "  Precision: 0.6405\n",
      "  Recall:    0.5915\n",
      "  F1 Score:  0.6150\n",
      "Adding validation score 62.97453148\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 12, last improved epoch 12, total epochs 12, n: 10, num_cycles: 24\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "  Train Acc: 64.23%\n",
      "  Val Acc: 63.48%\n",
      "  Precision: 0.6477\n",
      "  Recall:    0.5911\n",
      "  F1 Score:  0.6181\n",
      "Adding validation score 63.47909659\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 13, last improved epoch 13, total epochs 13, n: 10, num_cycles: 26\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n",
      "  Train Acc: 64.17%\n",
      "  Val Acc: 62.90%\n",
      "  Precision: 0.6221\n",
      "  Recall:    0.6574\n",
      "  F1 Score:  0.6393\n",
      "Adding validation score 62.90245074\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 14, last improved epoch 14, total epochs 14, n: 10, num_cycles: 28\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n",
      "  Train Acc: 64.19%\n",
      "  Val Acc: 62.85%\n",
      "  Precision: 0.6266\n",
      "  Recall:    0.6362\n",
      "  F1 Score:  0.6314\n",
      "Adding validation score 62.85439692\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 15, last improved epoch 15, total epochs 15, n: 10, num_cycles: 30\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n",
      "  Train Acc: 64.44%\n",
      "  Val Acc: 64.13%\n",
      "  Precision: 0.6167\n",
      "  Recall:    0.7468\n",
      "  F1 Score:  0.6755\n",
      "Adding validation score 64.12782316\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 16, last improved epoch 16, total epochs 16, n: 10, num_cycles: 32\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n",
      "  Train Acc: 64.70%\n",
      "  Val Acc: 63.74%\n",
      "  Precision: 0.6328\n",
      "  Recall:    0.6550\n",
      "  F1 Score:  0.6437\n",
      "Adding validation score 63.74339260\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 17, last improved epoch 17, total epochs 17, n: 10, num_cycles: 34\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n",
      "  Train Acc: 64.46%\n",
      "  Val Acc: 63.48%\n",
      "  Precision: 0.6387\n",
      "  Recall:    0.6209\n",
      "  F1 Score:  0.6296\n",
      "Adding validation score 63.47909659\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 18, last improved epoch 18, total epochs 18, n: 10, num_cycles: 36\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "  Train Acc: 64.72%\n",
      "  Val Acc: 62.69%\n",
      "  Precision: 0.6155\n",
      "  Recall:    0.6761\n",
      "  F1 Score:  0.6444\n",
      "Adding validation score 62.68620855\n",
      "Checking PAI switch with mode n, switch mode DOING_SWITCH_EVERY_TIME, epoch 19, last improved epoch 19, total epochs 19, n: 10, num_cycles: 38\n",
      "Returning True - switching every time\n",
      "Importing best Model for switch to PA...\n",
      "Switching back to N...\n",
      "Resetting committed to initial rate to False\n",
      "Saving model before starting normal training to retain PBNodes regardless of next N Phase results\n",
      "[DEBUG] Restructured Reset optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amatthews/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== FINAL TEST ==========\n",
      "Test Acc: 63.32%\n",
      "Precision: 0.6222\n",
      "Recall:    0.6779\n",
      "F1 Score:  0.6489\n",
      "Total dendrites added: 20\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from perforatedai import globals_perforatedai as GPA\n",
    "from perforatedai import utils_perforatedai as UPA\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIG\n",
    "# ==========================================================\n",
    "\n",
    "DATA_PATH = \"../data/processed/modis_firms_train_val_test_dataset.npz\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 512\n",
    "EPOCHS = 20\n",
    "LR = 0.01\n",
    "WEIGHT_DECAY = 0.0\n",
    "GAMMA = 0.7\n",
    "USE_DENDRITES = True   \n",
    "\n",
    "MAX_TRAIN = 20000\n",
    "MAX_VAL = 5000\n",
    "MAX_TEST = 5000\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f\"[DEBUG] Using device: {device}\")\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ==========================================================\n",
    "# MODEL\n",
    "# ==========================================================\n",
    "\n",
    "class FireNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD and BALANCE DATA\n",
    "# ==========================================================\n",
    "\n",
    "print(\"[DEBUG] Loading dataset...\")\n",
    "\n",
    "with np.load(DATA_PATH) as f:\n",
    "    X_train = f[\"X_train\"]\n",
    "    y_train = f[\"y_train\"]\n",
    "    X_val = f[\"X_val\"]\n",
    "    y_val = f[\"y_val\"]\n",
    "    X_test = f[\"X_test\"]\n",
    "    y_test = f[\"y_test\"]\n",
    "\n",
    "print(f\"[DEBUG] Original Train Size: {len(X_train):,}\")\n",
    "\n",
    "def balanced_sample(X, y, max_samples):\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    neg_idx = np.where(y == 0)[0]\n",
    "\n",
    "    n_each = max_samples // 2\n",
    "    n_each = min(n_each, len(pos_idx), len(neg_idx))\n",
    "\n",
    "    pos_sample = np.random.choice(pos_idx, n_each, replace=False)\n",
    "    neg_sample = np.random.choice(neg_idx, n_each, replace=False)\n",
    "\n",
    "    idx = np.concatenate([pos_sample, neg_sample])\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# Since the dataset is extremely imbalanced, applying balancing\n",
    "X_train, y_train = balanced_sample(X_train, y_train, MAX_TRAIN)\n",
    "X_val, y_val = balanced_sample(X_val, y_val, MAX_VAL)\n",
    "X_test, y_test = balanced_sample(X_test, y_test, MAX_TEST)\n",
    "\n",
    "# Debug class balance\n",
    "print(\"[DEBUG] Train positive ratio:\", np.mean(y_train))\n",
    "print(\"[DEBUG] Val positive ratio:\", np.mean(y_val))\n",
    "print(\"[DEBUG] Test positive ratio:\", np.mean(y_test))\n",
    "\n",
    "# ==========================================================\n",
    "# DATALOADERS\n",
    "# ==========================================================\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train).float(),\n",
    "                torch.tensor(y_train).float()),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_val).float(),\n",
    "                torch.tensor(y_val).float()),\n",
    "    batch_size=TEST_BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_test).float(),\n",
    "                torch.tensor(y_test).float()),\n",
    "    batch_size=TEST_BATCH_SIZE\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# TRAIN / EVAL\n",
    "# ==========================================================\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy(output, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = (output > 0.5).float()\n",
    "        correct += preds.eq(target.unsqueeze(1)).sum().item()\n",
    "\n",
    "    return 100.0 * correct / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            preds = (output > 0.5).cpu().numpy().flatten()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(target.numpy())\n",
    "\n",
    "    acc = np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    recall = recall_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "\n",
    "    return acc * 100, precision, recall, f1\n",
    "\n",
    "# ==========================================================\n",
    "# RUN EXPERIMENTS (BASELINE + DENDRITES)\n",
    "# ==========================================================\n",
    "\n",
    "for is_dendrite in [False, True]:\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"RUNNING WITH is_dendrite = {is_dendrite}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Reinitialize model each run\n",
    "    # ------------------------------------------------------\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    base_model = FireNN(input_dim)\n",
    "\n",
    "    if is_dendrite:\n",
    "        print(\"[DEBUG] Initializing WITH dendrites...\")\n",
    "        model = UPA.initialize_pai(base_model, save_name=\"fire_model\")\n",
    "    else:\n",
    "        print(\"[DEBUG] Running BASELINE...\")\n",
    "        model = base_model\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"[DEBUG] Parameter count: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # OPTIMIZER\n",
    "    # ------------------------------------------------------\n",
    "\n",
    "    if is_dendrite:\n",
    "        GPA.pc.set_testing_dendrite_capacity(False)\n",
    "        GPA.pc.set_weight_decay_accepted(True)\n",
    "        GPA.pc.set_verbose(False)\n",
    "\n",
    "        GPA.pai_tracker.set_optimizer(optim.Adam)\n",
    "        GPA.pai_tracker.set_scheduler(StepLR)\n",
    "\n",
    "        optim_args = {\"params\": model.parameters(), \"lr\": LR, \"weight_decay\": WEIGHT_DECAY}\n",
    "        sched_args = {\"step_size\": 1, \"gamma\": GAMMA}\n",
    "\n",
    "        optimizer, scheduler = GPA.pai_tracker.setup_optimizer(model, optim_args, sched_args)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # TRAIN LOOP\n",
    "    # ------------------------------------------------------\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "        train_acc = train_epoch()\n",
    "        val_acc, val_prec, val_rec, val_f1 = evaluate(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"  Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Precision: {val_prec:.4f}\")\n",
    "        print(f\"  Recall:    {val_rec:.4f}\")\n",
    "        print(f\"  F1 Score:  {val_f1:.4f}\")\n",
    "\n",
    "        if is_dendrite:\n",
    "            model, restructured, training_complete = \\\n",
    "                GPA.pai_tracker.add_validation_score(val_acc, model)\n",
    "\n",
    "            if restructured and not training_complete:\n",
    "                print(\"[DEBUG] Restructured Reset optimizer\")\n",
    "                optim_args = {\"params\": model.parameters(), \"lr\": LR, \"weight_decay\": WEIGHT_DECAY}\n",
    "                sched_args = {\"step_size\": 1, \"gamma\": GAMMA}\n",
    "                optimizer, scheduler = GPA.pai_tracker.setup_optimizer(model, optim_args, sched_args)\n",
    "\n",
    "            if training_complete:\n",
    "                print(\"[DEBUG] Training ended early by PAI.\")\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # TEST\n",
    "    # ------------------------------------------------------\n",
    "\n",
    "    test_acc, test_prec, test_rec, test_f1 = evaluate(test_loader)\n",
    "\n",
    "    print(\"\\n========== FINAL TEST ==========\")\n",
    "    print(f\"Test Acc: {test_acc:.2f}%\")\n",
    "    print(f\"Precision: {test_prec:.4f}\")\n",
    "    print(f\"Recall:    {test_rec:.4f}\")\n",
    "    print(f\"F1 Score:  {test_f1:.4f}\")\n",
    "\n",
    "    if is_dendrite:\n",
    "        print(\"Total dendrites added:\",\n",
    "            GPA.pai_tracker.member_vars[\"num_dendrites_added\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
